{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLOps pipeline in AzureDevOps will leverage the same pipeline steps created in the previous example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "os.makedirs('./pipeline_step_scripts', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./pipeline_step_scripts/get_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pipeline_step_scripts/get_data.py\n",
    "\n",
    "from azureml.core import Run, Workspace, Datastore, Dataset\n",
    "from azureml.data.datapath import DataPath\n",
    "import pandas as pd\n",
    "import os\n",
    "import argparse\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "#Parse input arguments\n",
    "parser = argparse.ArgumentParser(\"Get data from and register in AML workspace\")\n",
    "parser.add_argument('--exp_raw_data', dest='exp_raw_data', required=True)\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "exp_raw_dataset = args.exp_raw_data\n",
    "\n",
    "#Get current run\n",
    "current_run = Run.get_context()\n",
    "\n",
    "#Get associated AML workspace\n",
    "ws = current_run.experiment.workspace\n",
    "\n",
    "#Connect to default data store\n",
    "ds = ws.get_default_datastore()\n",
    "\n",
    "tab_data_set = Dataset.Tabular.from_delimited_files(path=(ds, 'diabetes-data/*.csv'))\n",
    "\n",
    "\n",
    "raw_df = tab_data_set.to_pandas_dataframe()\n",
    "\n",
    "#Make directory on mounted storage\n",
    "os.makedirs(exp_raw_dataset, exist_ok=True)\n",
    "\n",
    "#Upload modified dataframe\n",
    "raw_df.to_csv(os.path.join(exp_raw_dataset, 'exp_raw_data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./pipeline_step_scripts/split.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pipeline_step_scripts/split.py\n",
    "\n",
    "from azureml.core import Run, Workspace, Datastore, Dataset\n",
    "from azureml.data.datapath import DataPath\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from numpy.random import seed\n",
    "\n",
    "#Parse input arguments\n",
    "parser = argparse.ArgumentParser(\"Split raw data into train/test and scale appropriately\")\n",
    "\n",
    "parser.add_argument('--exp_training_data', dest='exp_training_data', required=True)\n",
    "parser.add_argument('--exp_testing_data', dest='exp_testing_data', required=True)\n",
    "\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "exp_training_data = args.exp_training_data\n",
    "exp_testing_data = args.exp_testing_data\n",
    "\n",
    "\n",
    "#Get current run\n",
    "current_run = Run.get_context()\n",
    "\n",
    "#Get associated AML workspace\n",
    "ws = current_run.experiment.workspace\n",
    "\n",
    "# Read input dataset to pandas dataframe\n",
    "raw_datset = current_run.input_datasets['Exp_Raw_Data']\n",
    "raw_df = raw_datset.to_pandas_dataframe()\n",
    "\n",
    "\n",
    "for col in raw_df.columns:\n",
    "    missing = raw_df[col].isnull()\n",
    "    num_missing = np.sum(missing)\n",
    "    if num_missing > 0:  \n",
    "        raw_df['quality_{}_ismissing'.format(col)] = missing\n",
    "\n",
    "\n",
    "print(raw_df.columns)\n",
    "\n",
    "#Split data into training set and test set\n",
    "df_train, df_test = train_test_split(raw_df, test_size=0.3, random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Save train data to both train and test (reflects the usage pattern in this sample. Note: test/train sets are typically distinct data).\n",
    "os.makedirs(exp_training_data, exist_ok=True)\n",
    "os.makedirs(exp_testing_data, exist_ok=True)\n",
    "\n",
    "df_train.to_csv(os.path.join(exp_training_data, 'exp_training_data.csv'), index=False)\n",
    "df_test.to_csv(os.path.join(exp_testing_data, 'exp_testing_data.csv'), index=False)\n",
    "\n",
    "# Save scaler to PipelineData and outputs for record-keeping\n",
    "#os.makedirs('./outputs', exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./pipeline_step_scripts/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pipeline_step_scripts/train.py\n",
    "\n",
    "from azureml.core import Run, Workspace, Datastore, Dataset\n",
    "from azureml.data.datapath import DataPath\n",
    "import os\n",
    "import argparse\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from numpy.random import seed\n",
    "\n",
    "\n",
    "#Parse input arguments\n",
    "parser = argparse.ArgumentParser(\"Train Logistic Regression model\")\n",
    "parser.add_argument('--exp_trained_model_pipeline_data', dest='exp_trained_model_pipeline_data', required=True)\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "exp_trained_model_pipeline_data = args.exp_trained_model_pipeline_data\n",
    "\n",
    "#Get current run\n",
    "run = Run.get_context()\n",
    "\n",
    "#Get associated AML workspace\n",
    "ws = run.experiment.workspace\n",
    "\n",
    "# Read input dataset to pandas dataframe\n",
    "X_train_dataset = run.input_datasets['Exp_Training_Data'].to_pandas_dataframe()\n",
    "X_test_dataset = run.input_datasets['Exp_Testing_Data'].to_pandas_dataframe()\n",
    "\n",
    "print(type(X_train_dataset))\n",
    "\n",
    "# Separate features and labels\n",
    "X_train, y_train = X_train_dataset[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].values, X_train_dataset['Diabetic'].values\n",
    "X_test, y_test   = X_test_dataset[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].values, X_test_dataset['Diabetic'].values\n",
    "\n",
    "\n",
    "\n",
    "# Set regularization hyperparameter\n",
    "reg = 0.01\n",
    "\n",
    "# Train a logistic regression model\n",
    "print('Training a logistic regression model with regularization rate of', reg)\n",
    "run.log('Regularization Rate',  np.float(reg))\n",
    "model = LogisticRegression(C=1/reg, solver=\"liblinear\").fit(X_train, y_train)\n",
    "\n",
    "# calculate accuracy\n",
    "y_hat = model.predict(X_test)\n",
    "acc = np.average(y_hat == y_test)\n",
    "print('Accuracy:', acc)\n",
    "run.log('Accuracy', np.float(acc))\n",
    "\n",
    "# calculate AUC\n",
    "y_scores = model.predict_proba(X_test)\n",
    "auc = roc_auc_score(y_test,y_scores[:,1])\n",
    "print('AUC: ' + str(auc))\n",
    "run.log('AUC', np.float(auc))\n",
    "\n",
    "run.parent.log(name='AUC', value=np.float(auc))\n",
    "run.parent.log(name='Accuracy', value=np.float(acc))\n",
    "\n",
    "# Save the trained model in the outputs folder\n",
    "os.makedirs('./outputs', exist_ok=True)\n",
    "joblib.dump(value=model, filename='./outputs/diabetes_model.pkl')\n",
    "\n",
    "#df_train.to_csv(os.path.join(exp_training_data, 'exp_training_data.csv'), index=False)\n",
    "os.makedirs(exp_trained_model_pipeline_data, exist_ok=True)\n",
    "\n",
    "shutil.copyfile('./outputs/diabetes_model.pkl', os.path.join(exp_trained_model_pipeline_data, 'diabetes_model.pkl'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./pipeline_step_scripts/evaluate_and_register.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pipeline_step_scripts/evaluate_and_register.py\n",
    "\n",
    "from azureml.core import Run, Workspace, Datastore, Dataset\n",
    "from azureml.core.model import Model\n",
    "from azureml.data.datapath import DataPath\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import shutil\n",
    "\n",
    "parser = argparse.ArgumentParser(\"Evaluate model and register if more performant\")\n",
    "parser.add_argument('--exp_trained_model_pipeline_data', type=str, required=True)\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "exp_trained_model_pipeline_data = args.exp_trained_model_pipeline_data\n",
    "\n",
    "\n",
    "#Get current run\n",
    "run = Run.get_context()\n",
    "\n",
    "#Get associated AML workspace\n",
    "ws = run.experiment.workspace\n",
    "\n",
    "#Get default datastore\n",
    "ds = ws.get_default_datastore()\n",
    "\n",
    "#Get metrics associated with current parent run\n",
    "metrics = run.get_metrics()\n",
    "\n",
    "print('current run metrics')\n",
    "for key in metrics.keys():\n",
    "        print(key, metrics.get(key))\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "print('parent run metrics')\n",
    "#Get metrics associated with current parent run\n",
    "metrics = run.parent.get_metrics()\n",
    "\n",
    "for key in metrics.keys():\n",
    "        print(key, metrics.get(key))\n",
    "print('\\n')\n",
    "\n",
    "current_model_AUC = float(metrics['AUC'])\n",
    "current_model_accuracy = float(metrics['Accuracy'])\n",
    "\n",
    "# Get current model from workspace\n",
    "model_name = 'diabetes_model'\n",
    "model_description = 'Diabetes model'\n",
    "model_list = Model.list(ws, name=model_name, latest=True)\n",
    "first_registration = len(model_list)==0\n",
    "\n",
    "updated_tags = {'AUC': current_model_AUC}\n",
    "\n",
    "print('updated tags')\n",
    "print(updated_tags)\n",
    "\n",
    "# Copy autoencoder training outputs to relative path for registration\n",
    "relative_model_path = 'model_files'\n",
    "run.upload_folder(name=relative_model_path, path=exp_trained_model_pipeline_data)\n",
    "\n",
    "\n",
    "#If no model exists register the current model\n",
    "if first_registration:\n",
    "    print('First model registration.')\n",
    "    #model = run.register_model(model_name, model_path='model_files', description=model_description, model_framework='sklearn', model_framework_version=tf.__version__, tags=updated_tags, datasets=formatted_datasets, sample_input_dataset = training_dataset)\n",
    "    run.register_model(model_path=relative_model_path, model_name='diabetes_model',\n",
    "                   tags=updated_tags,\n",
    "                   properties={'AUC': current_model_AUC})\n",
    "else:\n",
    "    #If a model has been registered previously, check to see if current model \n",
    "    #performs better. If so, register it.\n",
    "    print(dir(model_list[0]))\n",
    "    if float(model_list[0].tags['AUC']) < current_model_AUC:\n",
    "        print('New model performs better than existing model. Register it.')\n",
    "        #model = run.register_model(model_name, model_path='model_files', description=model_description, model_framework='Tensorflow/Keras', model_framework_version=tf.__version__, tags=updated_tags, datasets=formatted_datasets, sample_input_dataset = training_dataset)\n",
    "        run.register_model(model_path=relative_model_path, model_name='diabetes_model',\n",
    "                   tags=updated_tags,\n",
    "                   properties={'AUC': current_model_AUC, 'Accuracy': current_model_accuracy})\n",
    "    else:\n",
    "        print('New model does not perform better than existing model. Cancel run.')\n",
    "        run.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
